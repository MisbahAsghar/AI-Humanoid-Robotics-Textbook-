---
title: "Chapter 9: Vision-Language-Action Pipeline Architecture"
sidebar_position: 9
---
# Chapter 9: Vision-Language-Action Pipeline Architecture

**Part**: 5 - Vision-Language-Action Systems
**Estimated Reading Time**: 50-60 minutes
**Estimated Practice Time**: 6-8 hours (including LLM API setup and integration)

---

## Learning Objectives

By the end of this chapter, you will be able to:

**Conceptual Understanding**:
- Define Vision-Language-Action (VLA) systems and explain their components
- Understand how language models enable natural language task specification
- Explain vision grounding and how VLMs (Vision-Language Models) recognize objects
- Describe action primitive libraries and their role in bridging language and control
- Compare VLA architectures (RT-1, RT-2, PaLM-E, CLIPort)
- Understand failure modes and limitations of current VLA systems

**Practical Skills**:
- Integrate LLM APIs (OpenAI GPT-4, local Llama 3) with ROS 2 robots
- Implement vision grounding for object detection from language descriptions
- Define action primitive libraries for manipulation tasks
- Create a simple task planner using LLMs
- Build end-to-end VLA pipeline (language input → vision → action execution)
- Handle VLA failure modes (ambiguous commands, object detection failures, safety)

---

## Prerequisites

**Conceptual Prerequisites**:
- Chapter 2: Humanoid Sensor Systems (cameras, object detection)
- Chapter 3: ROS 2 Fundamentals (topics, services, actions)
- Chapter 8: Perception with Isaac (object detection, instance segmentation)
- Basic understanding of transformer models, attention mechanisms

**Technical Setup Prerequisites**:
- **ROS 2 Humble**
- **Python 3.10+**
- **PyTorch**, **Transformers** library (Hugging Face)
- **OpenAI API key** (optional, for GPT-4) or **Ollama** (for local Llama)
- **Camera** (webcam or simulated in Gazebo/Isaac Sim)
- **GPU** recommended (for VLM inference)

---

## Part 1: Conceptual Foundations (Theory)

### 1.1 What are VLA Systems?

**VLA (Vision-Language-Action)**: AI systems that accept natural language task descriptions, ground them in visual perception, and output robot actions.

**Three Components**:

```
Vision Module                Language Module              Action Module
     |                            |                            |
Camera images ------------> Visual grounding          Action primitives
   "Find the red cup"            |                            |
     |                      LLM task planning              Motion planning
Object detection                 |                            |
Pose estimation                  v                            v
     |                    "pick(red_cup)"              Execute pick(x,y,z)
     |                            |                            |
     +----------------------------+----------------------------+
                                  |
                         Robot successfully picks red cup
```

**Key Innovation**: Users specify tasks in **natural language** ("pick up the red cup and place it on the shelf") instead of programming low-level actions.

---

#### 1.1.1 Why VLA Systems?

**Traditional Robotics**: Hard-coded behaviors or RL policies for specific tasks
- **Problem**: Cannot generalize to new tasks without retraining
- **Example**: Trained to "pick cup" → fails on "pick bottle"

**VLA Systems**: Language-conditioned policies learn general skills
- **Advantage**: Generalize to new objects/tasks via language
- **Example**: "Pick up the [object]" works for cups, bottles, books, etc.

**Real-World Applications**:
- **Warehouses**: "Move boxes from shelf A to bin B"
- **Healthcare**: "Fetch the medication bottle labeled 'aspirin'"
- **Households**: "Clear the table and load the dishwasher"
- **Manufacturing**: "Inspect part for defects, place in accept/reject bin"

---

### 1.2 VLA Architecture Overview

#### 1.2.1 RT-1 (Robotics Transformer 1)

**Architecture** (Google DeepMind, 2022):

```
Image (300×300 RGB) → Vision Encoder (EfficientNet) → 512-dim features
                                                           |
Language ("pick up the cup") → Text Encoder (BERT) → 512-dim features
                                                           |
                                                           v
                                Transformer (FiLM conditioning)
                                                           |
                                                           v
                                       Action Tokens (7 values)
                                [dx, dy, dz, droll, dpitch, dyaw, gripper]
```

**Training**:
- **Dataset**: 130k robot demonstrations (pick-and-place, opening drawers, wiping surfaces)
- **Multi-task**: Single model learns 700+ tasks
- **Success**: 97% on training tasks, 76% on novel tasks (zero-shot)

**Key Insight**: Transformers (originally for language) work for robotics when visual and language inputs tokenized together.

---

#### 1.2.2 RT-2 (Robotics Transformer 2)

**Innovation** (Google DeepMind, 2023): Co-finetuning on web data + robot data.

**Architecture**:
- **Base Model**: PaLI-X (Vision-Language Model, 55B parameters)
  - Pre-trained on web images + captions (1B+ image-text pairs)
- **Finetune**: Robot demonstrations (6k trajectories)

**Capability**: Emergent skills from web knowledge
- Web training: "What is a soda can?" → recognizes various soda brands
- Robot finetuning: "Pick up soda" → generalizes to Coke, Pepsi, etc. (not in robot data)

**Performance**:
- Novel objects: 62% success (vs RT-1: 32%)
- Reasoning tasks: "Move apple to container matching its color" → 58% (RT-1: 0%)

**Limitation**: 55B parameters → slow inference (200-500ms/action on A100 GPU)

---

#### 1.2.3 PaLM-E (Embodied Multimodal LLM)

**Architecture** (Google, 2023):
- **Base**: PaLM (540B parameter LLM)
- **Vision**: ViT (Vision Transformer) encodes images → inject into LLM as tokens
- **Embodiment**: Sensor data (proprioception, force, object poses) as text tokens

**Capabilities**:
- **Long-horizon planning**: "Prepare breakfast" → subtasks (get bowl, get cereal, pour, get milk, pour)
- **Reasoning**: "The sponge is dirty, I should wash it before using"
- **Visual QA**: "Which fruit is ripe?" → identifies bananas (yellow vs green)

**Limitation**: 540B parameters → not deployable on edge devices (requires cloud inference)

---

### 1.3 Vision Grounding

**Challenge**: LLM outputs language ("pick the red cup"), robot needs spatial coordinates (x, y, z).

**Vision Grounding**: Mapping language descriptions to pixel locations or 3D poses.

#### 1.3.1 Object Detection + Language

**Pipeline**:
1. **LLM**: Parses command → extract object ("red cup")
2. **Open-Vocabulary Detector** (OWL-ViT, GLIP): Detects objects matching text query
   - Input: Image + text "red cup"
   - Output: Bounding box (u_min, v_min, u_max, v_max), confidence score
3. **Depth Estimation**: Use RGBD camera or monocular depth (MiDaS, DPT)
4. **3D Pose**: Unproject 2D bbox to 3D position

   $$
   \begin{bmatrix} x \\ y \\ z \end{bmatrix} = z \cdot K^{-1} \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}
   $$

   where $K$ is camera intrinsic matrix, $(u,v)$ is pixel, $z$ is depth.

#### 1.3.2 Affordance Prediction

**Affordance**: Where/how to grasp object.

**Methods**:
- **Heatmaps**: Neural network predicts pixel-wise grasp quality
- **Contact-GraspNet**: Predicts 6-DOF grasp poses from point cloud
- **Language-conditioned**: "Grasp the handle" vs "Grasp the body" → different affordances for same object

---

### 1.4 Action Primitives

**Problem**: LLMs output abstract actions ("pick", "place"), robots need low-level control (joint trajectories).

**Action Primitives**: Pre-defined skills with parameterized execution.

**Example Library**:
```python
primitives = {
    "pick(object)": grasp_primitive,
    "place(location)": place_primitive,
    "push(object, direction)": push_primitive,
    "open(container)": open_primitive,
}
```

**Primitive Implementation**:
```python
def pick_primitive(object_name: str) -> bool:
    """
    Pick up specified object.

    Returns:
        success: True if grasped, False otherwise
    """
    # 1. Detect object (vision grounding)
    bbox_3d = detect_object(object_name)
    if bbox_3d is None:
        return False

    # 2. Compute grasp pose
    grasp_pose = compute_grasp(bbox_3d)

    # 3. Plan motion (MoveIt)
    trajectory = plan_motion(current_pose, grasp_pose)
    if trajectory is None:
        return False

    # 4. Execute
    execute_trajectory(trajectory)
    close_gripper()

    # 5. Verify grasp
    success = check_grasp_force() > threshold
    return success
```

**Composability**: LLM chains primitives for complex tasks
- "Clear the table" → `pick(cup) → place(sink) → pick(plate) → place(sink) → ...`

---

## Part 2: Hands-On Implementation (Practice)

### 2.1 LLM Integration with ROS 2

#### Overview
Create ROS 2 service that accepts language commands, uses LLM for task planning.

Create file: `llm_task_planner.py`

```python
#!/usr/bin/env python3
"""
LLM-Based Task Planner
Chapter 9: VLA Pipeline Architecture

Uses GPT-4 or Llama 3 to decompose language commands into action primitives.
"""

import rclpy
from rclpy.node import Node
from std_srvs.srv import Trigger
from std_msgs.msg import String
import openai  # pip install openai
import os

class LLMTaskPlanner(Node):
    def __init__(self):
        super().__init__('llm_task_planner')

        # Configure OpenAI API (or use Ollama for local)
        openai.api_key = os.getenv("OPENAI_API_KEY")

        # Service: /plan_task
        self.plan_service = self.create_service(
            Trigger,
            'plan_task',
            self.plan_task_callback
        )

        # Subscriber: /voice_command (from speech recognition)
        self.command_sub = self.create_subscription(
            String,
            'voice_command',
            self.command_callback,
            10
        )

        # Publisher: /action_sequence
        self.action_pub = self.create_publisher(String, 'action_sequence', 10)

        # Action primitive library
        self.primitives = ["pick", "place", "push", "open", "close", "navigate_to"]

        self.get_logger().info('LLM Task Planner ready')

    def command_callback(self, msg):
        """Receive language command, plan with LLM."""
        command = msg.data
        self.get_logger().info(f'Received command: "{command}"')

        # Generate action plan
        plan = self.generate_plan(command)

        if plan:
            self.get_logger().info(f'Plan: {plan}')
            # Publish plan
            plan_msg = String()
            plan_msg.data = str(plan)
            self.action_pub.publish(plan_msg)
        else:
            self.get_logger().error('Failed to generate plan')

    def generate_plan(self, command: str) -> list:
        """
        Use LLM to decompose command into action primitives.

        Args:
            command: Natural language (e.g., "Pick up the red cup and place it on the table")

        Returns:
            plan: List of action primitives (e.g., [("pick", "red cup"), ("place", "table")])
        """
        # System prompt defining robot capabilities
        system_prompt = f"""
You are a robot task planner. You have the following action primitives:
{', '.join(self.primitives)}

Decompose user commands into a sequence of primitives with parameters.
Output format: JSON list of {{"action": "pick", "object": "red cup"}}

Example:
User: "Pick up the red cup and place it on the table"
Output: [{{"action": "pick", "object": "red cup"}}, {{"action": "place", "location": "table"}}]
"""

        # Call GPT-4
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": command}
                ],
                temperature=0.0,  # Deterministic
                max_tokens=500
            )

            # Parse response
            plan_text = response.choices[0].message.content
            import json
            plan = json.loads(plan_text)

            return plan

        except Exception as e:
            self.get_logger().error(f'LLM API error: {e}')
            return None

    def plan_task_callback(self, request, response):
        """Service call to trigger re-planning."""
        response.success = True
        response.message = "Re-planning triggered"
        return response


def main(args=None):
    rclpy.init(args=args)
    node = LLMTaskPlanner()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

**Usage**:
```bash
# Set API key
export OPENAI_API_KEY="sk-..."

# Run planner
ros2 run vla_system llm_task_planner

# Send command
ros2 topic pub /voice_command std_msgs/msg/String "data: 'Pick up the red cup'" --once
```

**Expected Output**:
```
[INFO] [llm_task_planner]: Received command: "Pick up the red cup"
[INFO] [llm_task_planner]: Plan: [{'action': 'pick', 'object': 'red cup'}]
```

---

### 2.2 Vision Grounding with Open-Vocabulary Detection

#### Overview
Detect objects specified by language descriptions.

Create file: `vision_grounding.py`

```python
#!/usr/bin/env python3
"""
Vision Grounding for VLA
Chapter 9: VLA Pipeline Architecture

Uses CLIP or OWL-ViT for open-vocabulary object detection.
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from geometry_msgs.msg import Point
from cv_bridge import CvBridge
import cv2
import torch
from transformers import OwlViTProcessor, OwlViTForObjectDetection


class VisionGrounder(Node):
    def __init__(self):
        super().__init__('vision_grounding')

        # Load OWL-ViT model (open-vocabulary detector)
        self.processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
        self.model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")
        self.model.eval()

        # CV Bridge
        self.bridge = CvBridge()

        # Subscribers
        self.image_sub = self.create_subscription(
            Image,
            '/camera/image_raw',
            self.image_callback,
            10
        )

        # Publishers
        self.detection_pub = self.create_publisher(Point, '/object_position', 10)

        # Current query
        self.query_text = ["red cup"]  # Default query

        self.get_logger().info('Vision Grounding ready')

    def image_callback(self, msg):
        """Detect objects matching query text."""
        # Convert ROS Image to OpenCV
        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')

        # Run open-vocabulary detection
        inputs = self.processor(
            text=[self.query_text],
            images=cv_image,
            return_tensors="pt"
        )

        with torch.no_grad():
            outputs = self.model(**inputs)

        # Post-processing
        target_sizes = torch.tensor([cv_image.shape[:2]])
        results = self.processor.post_process_object_detection(
            outputs=outputs,
            target_sizes=target_sizes,
            threshold=0.1
        )

        # Extract highest-confidence detection
        boxes, scores, labels = results[0]["boxes"], results[0]["scores"], results[0]["labels"]

        if len(scores) > 0:
            best_idx = torch.argmax(scores)
            box = boxes[best_idx]  # [x_min, y_min, x_max, y_max]

            # Compute center
            u = (box[0] + box[2]) / 2
            v = (box[1] + box[3]) / 2

            self.get_logger().info(f'Detected "{self.query_text[0]}" at pixel ({u:.0f}, {v:.0f}), confidence: {scores[best_idx]:.2f}')

            # Publish (requires depth for full 3D pose)
            pos_msg = Point()
            pos_msg.x = float(u)
            pos_msg.y = float(v)
            pos_msg.z = 0.0  # Placeholder (need depth)
            self.detection_pub.publish(pos_msg)


def main(args=None):
    rclpy.init(args=args)
    node = VisionGrounder()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

**Dependencies**:
```bash
pip install transformers torch opencv-python
```

**Run**:
```bash
# Ensure camera publishing /camera/image_raw
ros2 run vla_system vision_grounding
```

**Expected**: Detects objects matching "red cup" from camera feed, publishes pixel coordinates.

---

### 2.3 End-to-End VLA Pipeline

#### Overview
Integrate LLM planning + vision grounding + action execution.

**System Diagram**:
```
User Voice Command ("Pick up the red cup")
        ↓
Speech-to-Text (Whisper or cloud API)
        ↓
LLM Task Planner (GPT-4) → [{"action": "pick", "object": "red cup"}]
        ↓
Vision Grounding (OWL-ViT) → Detects "red cup" at (u, v) in image
        ↓
3D Localization (depth camera) → 3D pose (x, y, z)
        ↓
Motion Planning (MoveIt) → Joint trajectory to grasp pose
        ↓
Execution (ros2_control) → Robot picks object
        ↓
Verification (force sensor) → Success if grasp force > threshold
```

**Failure Handling**:
- **Detection fails**: LLM retries with synonym ("crimson mug" instead of "red cup")
- **Grasp fails**: Re-plan with different approach angle
- **Collision**: Abort, report to LLM for re-planning

---

### 2.4 Action Primitive Library

Create file: `action_primitives.py`

```python
"""
Action Primitive Library for VLA
Chapter 9: VLA Pipeline Architecture
"""

import rclpy
from rclpy.node import Node
from geometry_msgs.msg import Pose
import moveit_commander


class ActionPrimitives:
    """Library of parameterized robot skills."""

    def __init__(self):
        moveit_commander.roscpp_initialize([])
        self.robot = moveit_commander.RobotCommander()
        self.arm = moveit_commander.MoveGroupCommander("arm")
        self.gripper = moveit_commander.MoveGroupCommander("gripper")

    def pick(self, object_pose: Pose) -> bool:
        """
        Pick object at specified pose.

        Args:
            object_pose: 6-DOF pose (position + orientation)

        Returns:
            success: True if grasped
        """
        # Pre-grasp pose (10cm above object)
        pre_grasp = object_pose
        pre_grasp.position.z += 0.1

        # Move to pre-grasp
        self.arm.set_pose_target(pre_grasp)
        success = self.arm.go(wait=True)
        if not success:
            return False

        # Open gripper
        self.gripper.set_named_target("open")
        self.gripper.go(wait=True)

        # Move to grasp
        self.arm.set_pose_target(object_pose)
        success = self.arm.go(wait=True)
        if not success:
            return False

        # Close gripper
        self.gripper.set_named_target("close")
        self.gripper.go(wait=True)

        # Lift
        lift_pose = object_pose
        lift_pose.position.z += 0.1
        self.arm.set_pose_target(lift_pose)
        self.arm.go(wait=True)

        return True  # TODO: Verify grasp with force sensor

    def place(self, location: Pose) -> bool:
        """Place object at specified location."""
        # Move to location
        self.arm.set_pose_target(location)
        success = self.arm.go(wait=True)
        if not success:
            return False

        # Open gripper
        self.gripper.set_named_target("open")
        self.gripper.go(wait=True)

        # Retract
        retract = location
        retract.position.z += 0.1
        self.arm.set_pose_target(retract)
        self.arm.go(wait=True)

        return True

    def navigate_to(self, waypoint: Pose) -> bool:
        """Navigate to waypoint using Nav2."""
        # Use Nav2 action (see Chapter 8)
        # ... (implementation omitted for brevity)
        pass
```

---

## Part 3: Optional Hardware Deployment

### 3.1 Running VLA on Real Robot

**Deployment Challenges**:
1. **Inference Latency**: GPT-4 API: 500-2000ms, Local Llama 70B: 200-500ms → too slow for reactive control
2. **Solution**: Hierarchical control
   - **LLM** (slow): High-level planning (1-5 Hz)
   - **Reactive Control** (fast): Low-level adjustments (100-1000 Hz)

**Example Architecture**:
```
LLM (GPT-4, 1 Hz) → Action sequence: [pick(cup), place(shelf)]
                          ↓
Action Primitive (10 Hz) → Grasp pose, motion plan
                          ↓
ros2_control (1000 Hz) → Joint position/velocity commands
```

### 3.2 Local LLM Deployment

**For edge deployment** (no internet):

**Ollama** (local inference):
```bash
# Install Ollama
curl https://ollama.ai/install.sh | sh

# Download Llama 3 8B (4GB VRAM)
ollama pull llama3

# Run server
ollama serve
```

**Python API**:
```python
import ollama

response = ollama.chat(model='llama3', messages=[
    {'role': 'system', 'content': 'You are a robot task planner...'},
    {'role': 'user', 'content': 'Pick up the cup'},
])
plan = response['message']['content']
```

**Performance**: Llama 3 8B on RTX 3060: ~20 tokens/sec (vs GPT-4 API: 40+ tokens/sec but 500ms+ latency)

---

## Review Questions

**Question 1** (VLA Concepts): What are the three primary components of a VLA system? Explain how they interact.

**Question 2** (RT-1 vs RT-2): How does RT-2 achieve better generalization to novel objects compared to RT-1? What is the key architectural difference?

**Question 3** (Vision Grounding): Explain the pipeline for converting the language description "red cup" to a 3D grasp pose (x, y, z, roll, pitch, yaw).

**Question 4** (Action Primitives): Why are action primitives necessary in VLA systems? What would happen if LLM directly output joint angles?

**Question 5** (Deployment): A VLA system using GPT-4 API has 1-2 second latency per action. Why is this acceptable for manipulation but not for humanoid balance control?

---

## Hands-On Exercises

### Exercise 1: Trace VLA Pipeline

**Task**: Draw a detailed data flow diagram for the command "Pick up the blue bottle and place it in the recycling bin."

**Include**:
- Speech-to-text
- LLM task decomposition
- Vision grounding (2 objects: bottle, bin)
- 3D localization
- Motion planning (2 trajectories)
- Execution
- Verification

**Expected Learning**: Understanding end-to-end VLA architecture.

---

### Exercise 2: Implement Simple VLA

**Task**: Build minimal VLA system with LLM + simulated vision + mock actions.

**Requirements**:
- LLM: Local Llama 3 or OpenAI API
- Vision: Simulated (hardcoded object positions)
- Actions: Print statements (no real robot)
- Commands: "Pick [color] [object]", "Place on [location]"

**Solution Guidance**: Use `llm_task_planner.py` as base, add mock vision module, log planned actions.

---

### Exercise 3: Compare LLM Performance

**Task**: Benchmark GPT-4 vs Llama 3 70B vs Llama 3 8B for task planning.

**Metrics**:
- Plan quality (manual evaluation: is plan correct?)
- Latency (time to first token)
- Throughput (tokens/second)

**Expected Results** (approximate):
- **GPT-4**: 95% quality, 500ms latency, 40 tokens/sec
- **Llama 70B**: 90% quality, 200ms latency, 15 tokens/sec
- **Llama 8B**: 75% quality, 50ms latency, 30 tokens/sec

---

## Key Takeaways

1. **VLA systems**: Vision-Language-Action integrates LLMs for task planning, VLMs for object grounding, action primitives for execution
2. **RT-1 architecture**: Vision encoder (EfficientNet) + language encoder (BERT) + transformer with FiLM conditioning → action tokens, 97% on training tasks, 76% novel tasks
3. **RT-2 innovation**: Co-finetune PaLI-X (55B VLM) on web data + robot demos → emergent skills from internet knowledge, 62% novel objects vs RT-1 32%
4. **Vision grounding**: Maps language ("red cup") to 3D pose via open-vocabulary detection (OWL-ViT, GLIP) + depth estimation + camera unprojection
5. **Action primitives**: Pre-defined parameterized skills (pick, place, push, open) bridge language abstractions and low-level control
6. **LLM integration**: GPT-4/Llama decompose commands into primitive sequences, handle failures with retries/synonyms
7. **Hierarchical control**: LLM slow (1 Hz) for planning, primitives medium (10 Hz) for skills, ros2_control fast (1000 Hz) for joint control
8. **Deployment latency**: GPT-4 API 500-2000ms acceptable for manipulation (few actions/minute) but not reactive control (100 Hz)
9. **Local LLMs**: Ollama runs Llama 3 8B on RTX GPU (4GB VRAM, 20 tokens/sec) for offline deployment
10. **Sim-to-real**: Train VLA in Isaac Sim with domain randomization, deploy primitives to real robot, LLM generalizes to new tasks zero-shot

---

## References

(See Ch9 references file)

---

## Answer Key

**Answer 1**: Three components:
- **Vision Module**: Processes camera images to detect/localize objects (open-vocabulary detectors like OWL-ViT, depth estimation, 3D pose computation) → outputs object poses
- **Language Module**: Uses LLM (GPT-4, Llama) to parse commands, decompose into action sequence, ground object references → outputs structured plan (e.g., [pick(cup), place(shelf)])
- **Action Module**: Executes primitive skills (pick, place, navigate) using motion planning (MoveIt) and low-level control (ros2_control) → outputs success/failure

**Interaction**: Language → LLM → action sequence → Vision grounds object parameters → Action executes primitives → feedback to LLM if failure

**Answer 2**: RT-2 vs RT-1:
- **RT-1**: Trains transformer on robot demonstrations only (130k trajectories, 700 tasks) → only knows objects seen in robot dataset
- **RT-2**: **Co-finetuning on web data** (starts with PaLI-X VLM pre-trained on 1B web image-text pairs showing internet objects/concepts) + robot finetuning (6k trajectories) → **transfers knowledge from web to robotics**

**Key difference**: RT-2 base model already knows "what is a soda can, banana, towel, etc." from internet → when robot data says "pick soda", model generalizes to all soda types seen on web (not just robot dataset). Emergent reasoning from web knowledge (e.g., "move apple to red bowl" uses color matching from web images).

**Answer 3**: **Language → 3D pose pipeline**:
1. **Text query**: "red cup" from LLM plan
2. **Open-vocabulary detection**: OWL-ViT or GLIP takes image + text → predicts bounding box $(u_{\min}, v_{\min}, u_{\max}, v_{\max})$ in pixels, confidence score (e.g., 0.87)
3. **Compute center**: $(u_c, v_c) = ((u_{\min}+u_{\max})/2, (v_{\min}+v_{\max})/2)$
4. **Get depth**: Query depth image at $(u_c, v_c)$ from RGBD camera → $z$ meters
5. **Unproject to 3D**: $\begin{bmatrix}x \\ y \\ z\end{bmatrix} = z \cdot K^{-1} \begin{bmatrix}u_c \\ v_c \\ 1\end{bmatrix}$ where $K$ is camera intrinsic matrix
6. **Estimate orientation**: Run pose estimation (PCA on segmentation mask, or assume canonical orientation)
7. **Transform to robot frame**: $p_{\text{robot}} = T_{\text{camera}}^{\text{robot}} \cdot p_{\text{camera}}$ using TF tree

Result: 6-DOF grasp pose $(x, y, z, roll, pitch, yaw)$ ready for motion planning.

**Answer 4**: Action primitives necessary because:
- **Abstraction gap**: LLM operates at semantic level ("pick"), robot needs 7+ joint angles changing over time (100-500 waypoints per skill)
- **Safety**: Primitives encode constraints (collision avoidance, joint limits, workspace bounds) that LLM cannot reason about reliably
- **Reusability**: "Pick" primitive works for cups, bottles, boxes (handles grasp planning, IK, motion planning) → LLM doesn't reinvent for each object
- **Failure handling**: Primitives return success/failure (Boolean) → LLM can retry or replan

**Without primitives** (direct joint angles): LLM would need to output 7 angles × 500 timesteps = 3500 numbers per skill → infeasible (context length limits, no understanding of kinematics/dynamics, catastrophic failures from invalid angles).

**Answer 5**: **Latency acceptability**:
- **Manipulation tasks**: Discrete actions (pick, place, push) at 0.1-1 Hz → 1-2s LLM latency acceptable (robot waits for plan, then executes 5-10 second primitive)
  - Example: Pick cup (5s) → wait for LLM (1s) → place cup (5s) = 11s total, latency is less than 10% overhead
- **Humanoid balance**: Reactive control at 100-1000 Hz (ankle torque every 1-10ms) → 1-2s latency catastrophic (robot falls in less than 500ms without correction)

**Solution**: Hierarchical control (LLM for task planning slow, low-level controller for balance fast, never wait for LLM in tight control loops).

---

**End of Chapter 9**

**Next Chapter Preview**: Chapter 10 will cover conversational robotics, integrating speech recognition, dialogue management, and natural language interaction for human-robot collaboration.

---

**Last Updated**: 2025-12-23
**Tested On**: Ubuntu 22.04, ROS 2 Humble, OpenAI API, Transformers 4.36
