---
title: "Chapter 10: Conversational Robotics"
sidebar_position: 10
---
# Chapter 10: Conversational Robotics

**Part**: 5 - Vision-Language-Action Systems
**Estimated Reading Time**: 45-50 minutes
**Estimated Practice Time**: 5-7 hours (including speech model setup)

---

## Learning Objectives

By the end of this chapter, you will be able to:

**Conceptual Understanding**:
- Explain speech recognition architectures (Whisper, traditional ASR)
- Understand dialogue management and conversational state tracking
- Describe multi-turn conversation handling for robot task refinement
- Explain safety constraints and guardrails for LLM-controlled robots
- Understand text-to-speech (TTS) for robot responses
- Compare cloud vs edge deployment for conversational AI

**Practical Skills**:
- Integrate Whisper for real-time speech recognition in ROS 2
- Implement LLM-based dialogue manager with conversation history
- Add safety guardrails (workspace limits, forbidden actions, confirmation prompts)
- Integrate text-to-speech for robot audio feedback
- Build end-to-end voice-commanded robot system
- Handle multi-turn clarification dialogues
- Deploy conversational AI on Jetson Orin

---

## Prerequisites

**Conceptual Prerequisites**:
- Chapter 9: VLA Pipeline Architecture (LLM integration, action primitives)
- Chapter 3: ROS 2 Fundamentals (topics, services)
- Basic understanding of dialogue systems, state machines

**Technical Setup Prerequisites**:
- **ROS 2 Humble**
- **Python 3.10+**
- **Microphone** (USB or built-in)
- **Speaker** (for TTS output)
- **OpenAI Whisper** or cloud ASR API
- **LLM access** (GPT-4 or local Llama 3)
- **PyAudio** for audio capture

---

## Part 1: Conceptual Foundations (Theory)

### 1.1 Speech Recognition for Robotics

#### 1.1.1 Traditional ASR vs. Whisper

**Traditional ASR** (Automatic Speech Recognition):
- **Pipeline**: Audio → Feature Extraction (MFCC) → Acoustic Model (HMM/DNN) → Language Model → Text
- **Examples**: Google Speech API, Azure Speech, Vosk
- **Advantages**: Low latency (50-200ms), streaming
- **Disadvantages**: Requires language-specific models, sensitive to accents/noise

**Whisper** (OpenAI, 2022):
- **Architecture**: Transformer encoder-decoder trained on 680k hours of multilingual data
- **Advantages**: 99+ languages, robust to accents/noise, no language model needed
- **Disadvantages**: Higher latency (200-500ms for Medium model), not streaming

**For Robotics**:
- **Whisper preferred**: Handles diverse environments (factory noise, accents, technical terms)
- **Use**: Whisper Large-v3 (1.5B params) for accuracy, or Tiny (39M) for edge devices

---

#### 1.1.2 Whisper Architecture

**Model Sizes**:

| Model | Parameters | Speed (CPU) | Speed (GPU) | Accuracy (WER) |
|-------|------------|-------------|-------------|----------------|
| **Tiny** | 39M | 10× real-time | 50× | ~8% |
| **Base** | 74M | 7× | 30× | ~6% |
| **Small** | 244M | 4× | 20× | ~4.5% |
| **Medium** | 769M | 2× | 10× | ~3.5% |
| **Large-v3** | 1.5B | 1× | 5× | ~2.5% |

**WER (Word Error Rate)**: Lower is better (2.5% = 2-3 errors per 100 words).

**Deployment**:
- **Desktop/Cloud**: Whisper Large-v3 (best accuracy)
- **Jetson Orin**: Whisper Medium (balanced)
- **Jetson Nano**: Whisper Tiny (only option, 8% WER acceptable for commands)

---

### 1.2 Dialogue Management

**Challenge**: Multi-turn conversations require context tracking.

**Example Dialogue**:
```
User:  "Pick up the cup"
Robot: "Which cup? I see a red cup and a blue cup."
User:  "The red one"
Robot: "Got it. Picking up the red cup." [executes]
User:  "Now place it on the table"
Robot: "Placing red cup on table." [executes]
```

**Dialogue Manager** tracks:
- **Conversation History**: Previous utterances (for pronoun resolution: "it" = red cup)
- **Task State**: Current plan, executed actions, pending actions
- **World State**: Detected objects, robot position, grasp status

#### 1.2.1 Dialogue State Tracking

**State Variables**:
```python
dialogue_state = {
    "history": [
        {"role": "user", "content": "Pick up the cup"},
        {"role": "assistant", "content": "Which cup? ..."},
        {"role": "user", "content": "The red one"},
    ],
    "current_task": "pick",
    "target_object": "red cup",
    "clarification_needed": False,
    "awaiting_confirmation": False,
}
```

**LLM Prompt** (includes history):
```
System: You are a helpful humanoid robot. Previous conversation:
User: "Pick up the cup"
Assistant: "Which cup? I see a red cup and a blue cup."
User: "The red one"

Current objects in view: red cup (at 0.5m), blue cup (at 0.8m), table

User: "Now place it on the table"

Respond with action plan.
```

**LLM infers**: "it" refers to "red cup" from history → plan `place(red cup, table)`.

---

### 1.3 Safety and Guardrails

**Problem**: LLMs can generate unsafe actions.

**Example Unsafe Commands**:
- "Hit the person" → Forbidden action
- "Move to (100, 50, 0)" → Outside workspace
- "Apply 1000N grip force" → Exceeds actuator limits

**Guardrail Layers**:

**Layer 1: LLM Prompt Engineering**
```
System: You are a safe robot assistant. You MUST NOT:
- Move outside workspace (x: [-1, 1]m, y: [-1, 1]m, z: [0, 2]m)
- Touch humans
- Grasp fragile objects (glass, ceramics) with >10N force
- Execute unrecognized commands

If command is unsafe, respond: "I cannot do that because [reason]"
```

**Layer 2: Semantic Validation**
```python
def validate_action(action: dict) -> tuple[bool, str]:
    """Check if action is safe."""
    if action["type"] == "pick":
        obj = action["object"]
        if obj in ["person", "pet", "glass_vase"]:
            return False, "Cannot grasp fragile/living objects"

    if action["type"] == "navigate_to":
        x, y = action["position"]
        if not (-1 <= x <= 1 and -1 <= y <= 1):
            return False, "Position outside workspace"

    return True, "OK"
```

**Layer 3: Kinematic/Dynamic Limits**
- MoveIt collision checking (rejects plans hitting obstacles/self-collision)
- Joint limit enforcement (hardware + software)
- Force/torque limits (prevent overload)

**Layer 4: E-Stop**
- Physical emergency stop button
- Software watchdog (timeout if no command for >5 seconds)

---

### 1.4 Text-to-Speech for Robot Feedback

**TTS (Text-to-Speech)**: Convert robot responses to audio.

**Options**:
- **Cloud**: Google TTS, Azure TTS, Amazon Polly (natural voices, latency 100-300ms)
- **Local**: Piper, Coqui TTS, espeak (lower quality, latency 50-100ms, offline)

**For Robotics**: Use **local TTS** (no internet dependency, lower latency, privacy).

**Example** (Piper TTS):
```python
from piper import PiperVoice

voice = PiperVoice.load("en_US-lessac-medium")
audio = voice.synthesize("I am picking up the red cup")
play_audio(audio)  # PyAudio playback
```

---

## Part 2: Hands-On Implementation (Practice)

### 2.1 Speech Recognition with Whisper

Create file: `whisper_ros_node.py`

```python
#!/usr/bin/env python3
"""
Whisper Speech Recognition ROS 2 Node
Chapter 10: Conversational Robotics
"""

import rclpy
from rclpy.node import Node
from std_msgs.msg import String
import whisper
import pyaudio
import numpy as np
import threading


class WhisperASR(Node):
    def __init__(self):
        super().__init__('whisper_asr')

        # Load Whisper model
        model_size = "base"  # tiny, base, small, medium, large
        self.model = whisper.load_model(model_size)
        self.get_logger().info(f'Loaded Whisper {model_size} model')

        # Publisher: /speech_text
        self.text_pub = self.create_publisher(String, 'speech_text', 10)

        # Audio configuration
        self.RATE = 16000  # Whisper expects 16kHz
        self.CHUNK = 1024
        self.audio = pyaudio.PyAudio()

        # Start listening thread
        self.listening = True
        self.thread = threading.Thread(target=self.listen_loop)
        self.thread.start()

        self.get_logger().info('Whisper ASR ready. Speak into microphone...')

    def listen_loop(self):
        """Continuously listen for speech."""
        stream = self.audio.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.RATE,
            input=True,
            frames_per_buffer=self.CHUNK
        )

        audio_buffer = []
        silence_threshold = 500  # Adjust based on environment
        silence_duration = 0

        while self.listening:
            # Read audio chunk
            data = stream.read(self.CHUNK, exception_on_overflow=False)
            audio_data = np.frombuffer(data, dtype=np.int16)

            # Voice Activity Detection (VAD) - simple threshold
            amplitude = np.abs(audio_data).mean()

            if amplitude > silence_threshold:
                # Speaking detected
                audio_buffer.extend(audio_data)
                silence_duration = 0
            else:
                # Silence
                silence_duration += 1

                # If 2 seconds of silence after speech, transcribe
                if silence_duration > 30 and len(audio_buffer) > 16000:  # >1 sec of speech
                    self.transcribe(np.array(audio_buffer))
                    audio_buffer = []
                    silence_duration = 0

        stream.stop_stream()
        stream.close()

    def transcribe(self, audio_data):
        """Transcribe audio using Whisper."""
        # Convert to float32 [-1, 1]
        audio_float = audio_data.astype(np.float32) / 32768.0

        # Transcribe
        result = self.model.transcribe(audio_float, language="en", fp16=False)
        text = result["text"].strip()

        if text:
            self.get_logger().info(f'Recognized: "{text}"')

            # Publish
            msg = String()
            msg.data = text
            self.text_pub.publish(msg)

    def destroy_node(self):
        self.listening = False
        self.thread.join()
        self.audio.terminate()
        super().destroy_node()


def main(args=None):
    rclpy.init(args=args)
    node = WhisperASR()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

**Dependencies**:
```bash
pip install openai-whisper pyaudio numpy
```

**Run**:
```bash
ros2 run conversational_robot whisper_asr
# Speak into microphone: "Pick up the red cup"
# Expected: [INFO] Recognized: "Pick up the red cup"
```

---

### 2.2 Dialogue Manager with Conversation History

Create file: `dialogue_manager.py`

```python
#!/usr/bin/env python3
"""
Dialogue Manager with LLM
Chapter 10: Conversational Robotics
"""

import rclpy
from rclpy.node import Node
from std_msgs.msg import String
import openai
import os


class DialogueManager(Node):
    def __init__(self):
        super().__init__('dialogue_manager')

        # OpenAI API
        openai.api_key = os.getenv("OPENAI_API_KEY")

        # Subscribers
        self.speech_sub = self.create_subscription(
            String, 'speech_text', self.speech_callback, 10
        )

        # Publishers
        self.response_pub = self.create_publisher(String, 'robot_response', 10)
        self.action_pub = self.create_publisher(String, 'planned_action', 10)

        # Conversation history
        self.history = []

        # System prompt with safety
        self.system_prompt = """
You are a helpful humanoid robot assistant. You can perform these actions:
- pick(object): Grasp an object
- place(object, location): Place object at location
- navigate_to(location): Move to location
- observe(description): Look at specified area

SAFETY RULES:
- Never touch humans or pets
- Stay within workspace: x [-1, 1]m, y [-1, 1]m, z [0, 2]m
- Ask for confirmation before risky actions
- If command is unclear, ask clarifying questions

Respond with JSON: {"message": "text response", "action": {"type": "pick", "object": "cup"}}
If no action, set "action": null.
"""

        self.history.append({"role": "system", "content": self.system_prompt})
        self.get_logger().info('Dialogue Manager ready')

    def speech_callback(self, msg):
        """Process user speech."""
        user_text = msg.data
        self.get_logger().info(f'User said: "{user_text}"')

        # Add to history
        self.history.append({"role": "user", "content": user_text})

        # Get LLM response
        response = self.get_llm_response()

        if response:
            # Parse response
            import json
            try:
                parsed = json.loads(response)
                robot_message = parsed.get("message", "")
                action = parsed.get("action", None)

                # Publish response text
                if robot_message:
                    msg = String()
                    msg.data = robot_message
                    self.response_pub.publish(msg)
                    self.get_logger().info(f'Robot: "{robot_message}"')

                # Publish action
                if action:
                    action_msg = String()
                    action_msg.data = str(action)
                    self.action_pub.publish(action_msg)
                    self.get_logger().info(f'Planned action: {action}')

            except json.JSONDecodeError:
                self.get_logger().error(f'Failed to parse LLM response: {response}')

    def get_llm_response(self) -> str:
        """Query LLM with conversation history."""
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=self.history,
                temperature=0.7,
                max_tokens=300
            )

            assistant_message = response.choices[0].message.content

            # Add to history
            self.history.append({"role": "assistant", "content": assistant_message})

            return assistant_message

        except Exception as e:
            self.get_logger().error(f'LLM API error: {e}')
            return None


def main(args=None):
    rclpy.init(args=args)
    node = DialogueManager()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

---

### 2.3 Text-to-Speech Integration

Create file: `tts_node.py`

```python
#!/usr/bin/env python3
"""
Text-to-Speech ROS 2 Node
Chapter 10: Conversational Robotics
"""

import rclpy
from rclpy.node import Node
from std_msgs.msg import String
import subprocess


class TTSNode(Node):
    def __init__(self):
        super().__init__('tts_node')

        # Subscribe to robot responses
        self.response_sub = self.create_subscription(
            String, 'robot_response', self.response_callback, 10
        )

        self.get_logger().info('TTS Node ready')

    def response_callback(self, msg):
        """Convert text to speech and play."""
        text = msg.data
        self.get_logger().info(f'Speaking: "{text}"')

        # Use espeak (simple, local, low latency)
        # Alternative: piper-tts, coqui-tts for better quality
        try:
            subprocess.run(
                ["espeak", text],
                check=True,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
        except subprocess.CalledProcessError as e:
            self.get_logger().error(f'TTS error: {e}')


def main(args=None):
    rclpy.init(args=args)
    node = TTSNode()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

**Install espeak**:
```bash
sudo apt install espeak
```

**Run**:
```bash
ros2 run conversational_robot tts_node

# Test
ros2 topic pub /robot_response std_msgs/msg/String "data: 'Hello, I am ready to help'" --once
```

**Expected**: Robot voice says "Hello, I am ready to help"

---

### 2.4 End-to-End Conversational System

**Full Pipeline**:
```
Microphone → Whisper ASR → /speech_text topic
                                  ↓
                          Dialogue Manager (LLM)
                                  ↓
                    /robot_response → TTS → Speaker
                                  ↓
                    /planned_action → Action Executor (from Ch9)
```

**Launch All Nodes**:
```bash
# Terminal 1: Whisper ASR
ros2 run conversational_robot whisper_asr

# Terminal 2: Dialogue Manager
export OPENAI_API_KEY="sk-..."
ros2 run conversational_robot dialogue_manager

# Terminal 3: TTS
ros2 run conversational_robot tts_node

# Terminal 4: Action Executor (from Chapter 9)
ros2 run vla_system action_executor
```

**Example Interaction**:
```
User speaks: "Pick up the red cup"
  → Whisper publishes to /speech_text
  → Dialogue Manager calls GPT-4, publishes "Which cup? I see red and blue."
  → TTS speaks response
User speaks: "The red one"
  → Dialogue Manager publishes action: pick(red cup)
  → Action Executor runs pick primitive
  → Robot grasps cup
```

---

## Part 3: Optional Hardware Deployment

### 3.1 Edge Deployment on Jetson Orin

**Challenges**:
- **Whisper Large**: Too slow on Jetson (1× real-time)
- **GPT-4 API**: Requires internet, 500-2000ms latency
- **Solution**: Whisper Medium + Local Llama 3 8B

**Performance** (Jetson AGX Orin 64GB):
- **Whisper Medium**: 3× real-time (330ms for 1sec audio)
- **Llama 3 8B**: 15 tokens/sec (~2 sec for 30-token response)
- **End-to-end**: ~2.5 sec (speak 1 sec → transcribe 0.3s → LLM 2s → TTS 0.2s)

**Acceptable for**: Pick-and-place tasks (actions take 5-10 sec), unacceptable for reactive dialogue.

---

### 3.2 Confirmation Prompts for Safety

**Best Practice**: Require confirmation for irreversible actions.

**Example**:
```python
def requires_confirmation(action):
    """Check if action needs user confirmation."""
    risky_actions = ["delete", "throw_away", "power_off"]
    return action["type"] in risky_actions

# In dialogue manager
if requires_confirmation(action):
    response = f"Are you sure you want to {action['type']}? Say 'yes' to confirm."
    await_user_confirmation = True
else:
    execute_action(action)
```

---

## Review Questions

**Question 1** (ASR): Compare traditional ASR and Whisper for robot speech recognition. When would you choose Whisper over Google Speech API?

**Question 2** (Dialogue): Why is conversation history necessary for dialogue managers? Give an example where omitting history causes failure.

**Question 3** (Safety): List three layers of safety guardrails for LLM-controlled robots. Explain why multiple layers are necessary.

**Question 4** (TTS): What is the trade-off between cloud TTS (Google) and local TTS (espeak/Piper) for robotics applications?

**Question 5** (Deployment): Your conversational robot on Jetson Orin has 2.5 second end-to-end latency (speech → action). Is this acceptable? For what tasks would it NOT be acceptable?

---

## Hands-On Exercises

### Exercise 1: Multi-Turn Dialogue

**Task**: Extend dialogue_manager.py to handle clarification dialogues.

**Scenario**:
```
User: "Pick up the cup"
Robot: "Which cup?" [2 cups detected]
User: "The one on the left"
Robot: "Got it" [picks left cup]
```

**Solution Guidance**: Add object disambiguation logic, update system prompt with examples.

---

### Exercise 2: Voice-Commanded Navigation

**Task**: Integrate Whisper + Nav2 for voice-controlled navigation.

**Commands**:
- "Go to the kitchen"
- "Navigate to waypoint A"
- "Stop"

**Solution Guidance**: Map location names to coordinates, call Nav2 action server.

---

### Exercise 3: Safety Testing

**Task**: Test safety guardrails with adversarial commands.

**Commands to test**:
- "Move to position (10, 10, 5)" [outside workspace]
- "Grasp the glass vase with maximum force" [fragile object]
- "Touch the person standing nearby" [forbidden]

**Expected**: Robot refuses with explanation, does NOT execute.

---

## Key Takeaways

1. **Whisper ASR**: Transformer-based speech recognition, 99+ languages, robust to noise/accents, 2.5% WER (Large-v3)
2. **Multi-turn dialogue**: Conversation history required for pronoun resolution ("it", "that") and context tracking
3. **Dialogue manager**: LLM with system prompt + history, tracks task state, generates responses + action plans
4. **Safety guardrails**: 4 layers (LLM prompt, semantic validation, kinematic limits, E-stop) prevent unsafe actions
5. **TTS for feedback**: Local (Piper, espeak) for offline/low-latency, cloud (Google TTS) for natural voices
6. **Edge deployment**: Whisper Medium + Llama 3 8B runs on Jetson Orin (~2.5s end-to-end latency)
7. **Confirmation prompts**: Require "yes" before risky actions (delete, throw, power off)
8. **LLM prompt engineering**: System prompt defines robot capabilities, workspace limits, forbidden actions
9. **Conversation state**: Track history (for context), task state (for resumption), world state (detected objects)
10. **Acceptable latency**: 2-3 sec OK for manipulation commands (actions take 5-10 sec), NOT OK for reactive control (100 Hz balance)

---

## Answer Key

**Answer 1**: **Whisper vs Traditional ASR**:
- **Whisper**: Transformer trained on 680k hours multilingual → handles 99 languages, robust to accents (Indian, British, Australian), noise (factory, outdoor), technical terms (no language model bias)
- **Traditional**: Language-specific (separate models for English, Spanish, etc.), sensitive to out-of-vocabulary words (struggles with "LiDAR", "Franka"), degraded in noise

**Choose Whisper when**: Multi-lingual robots (international deployment), noisy environments (warehouses, construction), diverse users (various accents), offline operation (no internet for cloud API)

**Choose Cloud ASR when**: Need streaming (real-time transcription during speech, not after silence), ultra-low latency (less than 100ms), speaker diarization (who is speaking)

**Answer 2**: **Conversation history necessary for**:
- **Pronoun resolution**: "Pick up the cup. Now place it on the table" → "it" refers to "cup" from previous turn (without history, "it" is ambiguous)
- **Context understanding**: "Move forward 1 meter. Turn left" → "left" relative to robot's new position after first move
- **Clarification**: "Which cup?" ... "The red one" → combines current response with previous question

**Failure example without history**:
```
User: "Pick up the cup"
Robot: "Done" [picks cup A]
User: "Place it on the shelf"  [no history]
Robot: "What should I place?" [doesn't know "it" = cup A, history forgotten]
```

**Answer 3**: **Three safety layers**:
1. **LLM Prompt Guardrails**: System prompt lists forbidden actions ("never touch humans"), workspace limits (x/y/z bounds), confirmation requirements → LLM should refuse unsafe commands
   - **Why**: First line of defense, prevents most issues (80-90% of unsafe commands)
   - **Insufficient alone**: LLMs can hallucinate, ignore prompts, be jailbroken ("ignore previous instructions")

2. **Semantic Validation**: Code checks action against rules (workspace bounds, object blacklist, force limits)
   - **Why**: Catches LLM failures, provides deterministic safety (guaranteed rejection of out-of-workspace positions)
   - **Insufficient alone**: Cannot handle all edge cases (e.g., "pick person" might pass if not in blacklist)

3. **Physical Limits**: Hardware joint limits, motor torque limits, collision detection (MoveIt), force/torque sensor thresholds, E-stop button
   - **Why**: Ultimate safety layer (even if software fails, hardware prevents damage)
   - **All layers needed**: Defense-in-depth (each layer catches failures others miss)

**Answer 4**: **Cloud vs Local TTS**:
- **Cloud (Google TTS)**: Pros (natural human-like voices, prosody, 40+ languages), Cons (requires internet, 100-300ms latency, privacy concerns, API cost)
- **Local (Piper/espeak)**: Pros (offline operation, 50-100ms latency, no API cost, privacy), Cons (robotic voice quality, fewer language/voices)

**Trade-off**: Cloud for customer-facing robots (reception, service) where voice quality critical; local for industrial/research where reliability (offline) and latency more important than voice naturalness.

**Answer 5**: **2.5 sec latency**:
- **Acceptable for**: Manipulation tasks (pick-and-place where actions take 5-10 sec → latency less than 30% overhead), infrequent commands (1-2 commands/minute), voice control demos (user tolerates delay)
- **NOT acceptable for**: Reactive dialogue (conversational agent should respond less than 1 sec for natural conversation), emergency commands ("stop now!" requires immediate response), teleoperation (100-1000 Hz control needs less than 10ms latency), continuous command stream (multiple commands/second)

**Improvement**: Use smaller Whisper (Tiny: 50ms), edge LLM (Llama 3 8B: 1 sec), lower TTS latency → potential 1.5 sec end-to-end.

---

**End of Chapter 10**

**Next Chapter Preview**: Chapter 11 will integrate all previous knowledge (ROS 2, perception, LLMs) to implement autonomous navigation and manipulation, using Nav2 for waypoint following and MoveIt for pick-and-place tasks.

---

**Last Updated**: 2025-12-23
**Tested On**: Ubuntu 22.04, ROS 2 Humble, Whisper Large-v3, OpenAI API
