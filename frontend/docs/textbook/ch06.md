---
title: "Chapter 6: Unity Digital Twins"
sidebar_position: 6
---
# Chapter 6: Unity Digital Twins

**Part**: 3 - Simulation and Virtual Environments
**Estimated Reading Time**: 40-45 minutes
**Estimated Practice Time**: 5-6 hours (including Unity installation and setup)

---

## Learning Objectives

By the end of this chapter, you will be able to:

**Conceptual Understanding**:
- Define digital twins and explain their role in robotics
- Compare Unity and Gazebo for robotics simulation (physics vs. visualization)
- Understand real-time visualization vs. recorded playback use cases
- Explain the Unity Robotics Hub architecture (ROS-TCP Connector)
- Describe photorealistic rendering benefits for computer vision development

**Practical Skills**:
- Install Unity Hub and Unity Editor (2021.3 LTS or newer)
- Import URDF models into Unity using URDF Importer
- Set up ROS-TCP Connector for Unity-ROS 2 communication
- Visualize robot state from ROS 2 topics in Unity in real-time
- Create photorealistic environments with lighting and materials
- Record and replay robot trajectories for visualization
- Integrate Unity with Gazebo (hybrid simulation)

---

## Prerequisites

**Conceptual Prerequisites**:
- Chapter 3: ROS 2 Fundamentals (topics, services)
- Chapter 4: URDF and Robot Modeling (robot descriptions)
- Chapter 5: Gazebo Simulation (physics-based simulation)
- Basic understanding of 3D graphics (rendering, cameras, lighting)

**Technical Setup Prerequisites**:
- **Unity Hub** + **Unity Editor 2021.3 LTS** (or 2022.3 LTS)
- **ROS 2 Humble** installed
- **Windows 10/11, macOS, or Linux** (Unity is cross-platform)
- **GPU**: NVIDIA/AMD recommended (Intel integrated GPU acceptable)
- **Disk**: 20GB free space for Unity
- **RAM**: 8GB minimum (16GB recommended)

---

## Part 1: Conceptual Foundations (Theory)

### 1.1 What is a Digital Twin?

**Digital Twin**: A virtual representation of a physical system that mirrors its state, behavior, and environment in real-time or near-real-time.

#### 1.1.1 Digital Twin Components

**Three Elements**:
1. **Physical Asset**: Real robot hardware
2. **Virtual Model**: 3D simulation (Unity, Gazebo, Isaac Sim)
3. **Data Connection**: Bidirectional communication (sensor data → virtual, commands → physical)

**Data Flow**:
```
Physical Robot                    Digital Twin (Unity)
    |                                    |
    |---- Sensor Data (joint states) --->|
    |                                    | (Update 3D model)
    |                                    | (Render visualization)
    |<--- Commands (test motions) -------|
    |                                    |
```

**Use Cases**:
- **Monitoring**: Visualize robot state in remote locations (warehouse robot fleet)
- **Training**: Demonstrate robot capabilities to stakeholders (photorealistic rendering)
- **Debugging**: Replay recorded trajectories to diagnose failures
- **Predictive Maintenance**: Detect anomalies by comparing physical vs. digital behavior
- **Planning**: Test motions in virtual twin before executing on real robot

#### 1.1.2 Why Unity for Digital Twins?

**Unity Strengths**:
1. **Photorealistic Rendering**: HDRP (High Definition Render Pipeline), ray tracing, global illumination
2. **Cross-Platform**: Windows, macOS, Linux, WebGL (browser), mobile
3. **Game Engine Ecosystem**: Asset store (3D models, environments), animation tools
4. **Performance**: Optimized rendering (60-90 FPS on consumer GPUs)
5. **AR/VR Support**: Mixed reality visualization (HoloLens, Quest)

**Unity Weaknesses** (vs. Gazebo):
- **Physics**: Less accurate for contact-rich tasks (PhysX vs. ODE/Bullet/DART)
- **ROS Integration**: Requires third-party tools (Unity Robotics Hub)
- **Robot-Specific**: Gazebo has robot-specific plugins (differential drive, joint control)

**Ideal Use**: Unity for **visualization**, Gazebo for **physics simulation** → hybrid approach.

---

### 1.2 Unity vs. Gazebo: When to Use Each

| Aspect | Unity | Gazebo |
|--------|-------|--------|
| **Primary Purpose** | Visualization, rendering, training | Physics simulation, testing |
| **Rendering Quality** | Photorealistic (HDRP, ray tracing) | Functional (OGRE, basic lighting) |
| **Physics Accuracy** | Good (PhysX) | Excellent (ODE, Bullet, DART) |
| **ROS Integration** | Via ROS-TCP Connector | Native (gazebo_ros_pkgs) |
| **Real-Time Performance** | 60+ FPS (game engine optimized) | 20-30 FPS (physics-limited) |
| **AR/VR Support** | Native (HoloLens, Quest, WebXR) | None |
| **Use Case** | Demos, monitoring, CV dataset generation | Algorithm dev, control testing |
| **Learning Curve** | Moderate (game engine concepts) | Low (ROS-native) |
| **License** | Free (Personal), paid (Pro/Enterprise) | Open-source (Apache 2.0) |

**Decision Matrix**:
- **Need photorealism?** → Unity
- **Need accurate physics?** → Gazebo
- **Need both?** → Hybrid (Gazebo physics, Unity visualization)
- **Need AR/VR?** → Unity
- **ROS-native workflow?** → Gazebo

---

### 1.3 Unity Robotics Hub Architecture

**Unity Robotics Hub**: Official Unity toolkit for ROS integration.

**Components**:

1. **ROS-TCP Connector** (Unity side)
   - Unity plugin (C#) that connects to ROS 2 via TCP
   - Subscribes to ROS topics, publishes Unity data
   - Runs in Unity Editor or standalone builds

2. **ROS-TCP Endpoint** (ROS 2 side)
   - Python node that bridges ROS 2 DDS ↔ Unity TCP
   - Translates ROS messages to Unity-compatible JSON
   - Runs on robot computer or development machine

**Architecture**:
```
ROS 2 (Robot)                  Unity (Visualization)
    |                                |
 Nodes publish                       |
 /joint_states                       |
    |                                |
    v                                |
ROS-TCP Endpoint (Python)            |
    |                                |
    |---- TCP Socket (port 10000) -->|
    |                                |
    |                            ROS-TCP Connector (C#)
    |                                |
    |                            Update 3D robot model
    |                            Render photorealistic scene
```

**Why TCP Instead of DDS?** Unity runs on Windows/macOS where DDS discovery can be problematic → TCP provides cross-platform, firewall-friendly communication.

---

### 1.4 Photorealistic Rendering for Computer Vision

**Motivation**: Training vision models on real images is expensive (labeling cost). Unity can generate **synthetic labeled data** at scale.

**Unity Perception Package**:
- Automatic annotation (bounding boxes, segmentation masks, keypoints)
- Domain randomization (lighting, textures, object placement)
- Ground truth labels (perfect 3D pose, depth, normals)

**Example**: Generate 10,000 labeled images of robot manipulating objects
- Unity randomizes: object positions, colors, lighting, backgrounds
- Outputs: RGB images + JSON labels (object ID, 2D/3D bbox, pose)
- Train object detector (YOLO, Faster R-CNN) without manual labeling

**Sim-to-Real Gap**:
- **Challenge**: Models trained on synthetic data may not generalize to real images
- **Mitigation**: Photo-realistic rendering (HDRP), domain randomization, fine-tuning on small real dataset

---

## Part 2: Hands-On Implementation (Practice)

### 2.1 Installing Unity for Robotics

#### 2.1.1 Install Unity Hub

**Download** (cross-platform):
- Visit: [https://unity.com/download](https://unity.com/download)
- Download Unity Hub installer
- Install (default options)

**Unity Hub**: Manages Unity Editor versions, licenses, projects.

#### 2.1.2 Install Unity Editor

**In Unity Hub**:
1. Go to "Installs" tab
2. Click "Install Editor"
3. Choose **Unity 2021.3 LTS** (Long-Term Support, stable)
   - **Note**: Unity Robotics Hub supports 2020.3+, 2021.3 LTS recommended
4. Add modules:
   - **Linux Build Support** (if building for Linux robot)
   - **WebGL Build Support** (optional, for browser visualization)
5. Install (~5GB download)

**Verify Installation**:
- Open Unity Hub → Projects → New Project
- Template: **3D (URP)** or **3D (HDRP)** for photorealism
- Create test project, should open Unity Editor

#### 2.1.3 Install Unity Robotics Hub

**In Unity Editor**:
1. Window → Package Manager
2. Click "+" → "Add package from git URL"
3. Enter: `https://github.com/Unity-Technologies/Unity-Robotics-Hub.git?path=/com.unity.robotics.ros-tcp-connector`
4. Wait for import (~2 minutes)
5. Repeat for URDF Importer: `https://github.com/Unity-Technologies/URDF-Importer.git?path=/com.unity.robotics.urdf-importer`

**Verify**:
- Top menu should show "Robotics" menu
- Robotics → ROS Settings opens configuration window

---

### 2.2 Importing URDF into Unity

#### Step 1: Prepare URDF

Use URDF from Chapter 4 (e.g., `simple_humanoid.urdf`). Ensure:
- All `<mesh>` paths use absolute paths or ROS package URIs
- Materials defined (Unity will import colors)

#### Step 2: Import URDF

**In Unity Editor**:
1. **Assets → Import Robot from URDF**
2. Select `simple_humanoid.urdf` file
3. Configure import settings:
   - **Mesh Decomposer**: Vhacd (for collision)
   - **Axis Type**: Y Axis (Unity uses Y-up, ROS uses Z-up)
   - **Override Mesh**: Optional (replace with Unity primitives)
4. Click "Import URDF"

**Expected Result**: Robot appears in Scene view and Hierarchy panel as GameObject tree.

**Unity GameObject Hierarchy** (from URDF):
```
simple_humanoid (root)
├── torso
│   ├── head
│   ├── left_upper_arm
│   │   └── left_forearm
│   └── right_upper_arm
│       └── right_forearm
```

Each link becomes a GameObject with:
- **Mesh Renderer**: Visual geometry
- **Collider**: Collision geometry
- **Articulation Body**: Joint physics (Unity's joint system)

---

### 2.3 Setting Up ROS-Unity Communication

#### Step 1: Install ROS-TCP Endpoint (ROS 2 Side)

```bash
cd ~/ros2_ws/src
git clone https://github.com/Unity-Technologies/ROS-TCP-Endpoint.git
cd ~/ros2_ws
colcon build --packages-select ros_tcp_endpoint
source install/setup.bash
```

#### Step 2: Launch ROS-TCP Endpoint

```bash
ros2 run ros_tcp_endpoint default_server_endpoint --ros-args -p ROS_IP:=0.0.0.0
```

**Expected**:
```
[INFO] Starting server on 0.0.0.0:10000
[INFO] Waiting for connections...
```

**Parameters**:
- `ROS_IP`: IP address to listen on (`0.0.0.0` = all interfaces)
- Default port: `10000`

#### Step 3: Configure Unity ROS Settings

**In Unity Editor**:
1. **Robotics → ROS Settings**
2. Set **ROS IP Address**: `127.0.0.1` (localhost) or robot IP if remote
3. Set **ROS Port**: `10000`
4. Protocol: **ROS 2**
5. Click "Save"

#### Step 4: Test Connection

**Create test publisher** (ROS 2 side):
```bash
# Publish joint states at 10 Hz
ros2 topic pub /joint_states sensor_msgs/msg/JointState \
  "header: {frame_id: ''} \
   name: ['neck_pan', 'left_shoulder_pitch', 'left_elbow'] \
   position: [0.5, 0.0, 1.0] \
   velocity: [] \
   effort: []" \
  -r 10
```

**In Unity**: Robot should move to match published joint positions.

---

### 2.4 Practice Example: Real-Time Robot Visualization

#### Overview
Visualize humanoid robot state from ROS 2 topics in Unity.

Create C# script: `Assets/Scripts/RobotStateVisualizer.cs`

```csharp
using UnityEngine;
using Unity.Robotics.ROSTCPConnector;
using RosMessageTypes.Sensor;

/// <summary>
/// Subscribes to /joint_states topic and updates robot joints in Unity.
/// Chapter 6: Unity Digital Twins
/// </summary>
public class RobotStateVisualizer : MonoBehaviour
{
    [Header("ROS Configuration")]
    public string jointStateTopic = "/joint_states";

    [Header("Robot Joints")]
    public ArticulationBody neckJoint;
    public ArticulationBody leftShoulderJoint;
    public ArticulationBody leftElbowJoint;
    public ArticulationBody rightShoulderJoint;
    public ArticulationBody rightElbowJoint;

    private ROSConnection ros;

    void Start()
    {
        // Connect to ROS
        ros = ROSConnection.GetOrCreateInstance();
        ros.Subscribe<JointStateMsg>(jointStateTopic, UpdateJointStates);

        Debug.Log($"Subscribed to {jointStateTopic}");
    }

    void UpdateJointStates(JointStateMsg msg)
    {
        // Map ROS joint names to Unity ArticulationBody components
        for (int i = 0; i < msg.name.Length; i++)
        {
            string jointName = msg.name[i];
            float position = (float)msg.position[i];  // Radians

            // Update corresponding Unity joint
            if (jointName == "neck_pan" && neckJoint != null)
            {
                SetJointPosition(neckJoint, position);
            }
            else if (jointName == "left_shoulder_pitch" && leftShoulderJoint != null)
            {
                SetJointPosition(leftShoulderJoint, position);
            }
            else if (jointName == "left_elbow" && leftElbowJoint != null)
            {
                SetJointPosition(leftElbowJoint, position);
            }
            else if (jointName == "right_shoulder_pitch" && rightShoulderJoint != null)
            {
                SetJointPosition(rightShoulderJoint, position);
            }
            else if (jointName == "right_elbow" && rightElbowJoint != null)
            {
                SetJointPosition(rightElbowJoint, position);
            }
        }
    }

    void SetJointPosition(ArticulationBody joint, float targetPosition)
    {
        // Unity ArticulationBody uses degrees for revolute joints
        var drive = joint.xDrive;
        drive.target = targetPosition * Mathf.Rad2Deg;  // Convert rad → deg
        joint.xDrive = drive;
    }
}
```

#### Usage

1. **Attach script** to robot root GameObject in Unity
2. **Assign joints** in Inspector:
   - Drag `neck_pan` ArticulationBody to `neckJoint` field
   - Repeat for shoulder, elbow joints
3. **Play** in Unity Editor (press Play button)
4. **Publish joint states** from ROS 2:
   ```bash
   ros2 run ros_tcp_endpoint default_server_endpoint &
   ros2 topic pub /joint_states sensor_msgs/msg/JointState "{...}" -r 10
   ```

**Expected**: Robot in Unity moves to match ROS joint positions in real-time.

---

### 2.5 Creating Photorealistic Environments

#### 2.5.1 Lighting Setup

**Unity HDRP** (High Definition Render Pipeline) for photorealism:

1. **Skybox** (ambient lighting):
   - Window → Rendering → Lighting
   - Environment → Skybox Material: Select HDRI skybox
   - Download free HDRIs: [https://polyhaven.com/hdris](https://polyhaven.com/hdris)

2. **Directional Light** (sun):
   - Hierarchy → Create → Light → Directional Light
   - Intensity: 100,000 lux (daylight)
   - Color: Slight warm tint (255, 250, 240)
   - Enable shadows: Hard Shadows or Soft Shadows

3. **Point Lights** (indoor):
   - Simulate room lights, lamps
   - Intensity: 1,000-10,000 lux
   - Range: 5-10m

**Light Probes**: For realistic indirect lighting
- Create Light Probe Group
- Place probes throughout scene (1-2m spacing)
- Bake lighting: Window → Rendering → Lighting → Generate Lighting

#### 2.5.2 Materials and Textures

**PBR (Physically-Based Rendering)** materials:
- **Albedo**: Base color (diffuse)
- **Metallic**: 0 = dielectric (plastic, wood), 1 = metal (steel, aluminum)
- **Smoothness**: 0 = rough (concrete), 1 = glossy (polished metal)
- **Normal Map**: Surface detail without extra geometry

**Example**: Robot body material
- Albedo: Light gray (200, 200, 200)
- Metallic: 0.8 (aluminum chassis)
- Smoothness: 0.6 (brushed metal)

**Free Texture Sources**:
- Poly Haven: [https://polyhaven.com/textures](https://polyhaven.com/textures)
- Quixel Megascans: [https://quixel.com/megascans](https://quixel.com/megascans) (free with Epic account)

---

### 2.6 Recording and Playback

#### Use Case: Debugging Robot Failures

**Workflow**:
1. **Record** robot trajectory using ROS 2 bags:
   ```bash
   ros2 bag record /joint_states /tf /tf_static /camera/image_raw
   ```

2. **Playback** in Unity:
   ```bash
   ros2 bag play recorded_trajectory.db3
   ros2 run ros_tcp_endpoint default_server_endpoint
   ```

3. **Unity** subscribes to `/joint_states`, updates robot visualization

4. **Slow-motion playback**: `ros2 bag play --rate 0.1` (10× slower)

**Benefits**:
- Scrub through timeline to find failure moment
- Visualize from multiple camera angles
- Overlay sensor data (LiDAR points, camera frustum)

---

## Part 3: Optional Hardware Deployment

### 3.1 Hybrid Simulation (Gazebo + Unity)

**Concept**: Use Gazebo for physics, Unity for visualization simultaneously.

**Architecture**:
```
Gazebo (Physics)                Unity (Rendering)
     |                               |
     |--- /joint_states -----------→|
     |--- /scan --------------------→|
     |--- /camera/image (optional)->|
     |                               |
     |←-- /cmd_vel (optional) -------|
```

**Setup**:
1. Launch Gazebo with robot (physics simulation running)
2. Launch ROS-TCP Endpoint
3. Unity subscribes to Gazebo-published topics
4. Both visualizations update simultaneously (Gazebo functional, Unity photorealistic)

**Use Case**: Develop control algorithm in Gazebo (accurate physics), present results in Unity (stakeholder demo).

---

### 3.2 Web-Based Digital Twin (WebGL)

**Deployment**: Build Unity project as WebGL, host on web server.

**Steps**:
1. **File → Build Settings**
2. Select **WebGL** platform
3. Click "Build" → generates HTML/JS files
4. Host on server: `python -m http.server 8000`
5. Access: `http://localhost:8000`

**Use Case**: Remote monitoring dashboard (view robot fleet from browser without installing software).

**Limitation**: WebGL has performance constraints (30-60 FPS max, no multithreading).

---

### 3.3 AR/VR Visualization

**Concept**: View digital twin in augmented/virtual reality.

**AR Example** (HoloLens):
- Overlay virtual robot on real environment
- Visualize planned trajectories before execution
- Use case: Human-robot collaboration (show robot's next move)

**VR Example** (Meta Quest):
- Immersive robot teleoperation interface
- First-person view from robot's camera
- 3D spatial awareness for manipulation tasks

**Unity XR Setup**:
1. Install XR Plugin Management (Package Manager)
2. Select target platform (Oculus, OpenXR, ARCore, ARKit)
3. Configure XR settings
4. Build and deploy to headset

**Note**: AR/VR setup beyond this chapter's scope; see Unity XR docs.

---

## Review Questions

**Question 1** (Concepts): What are the three components of a digital twin? Explain the data flow between physical and virtual systems.

**Question 2** (Comparison): When would you choose Unity over Gazebo for a robotics project? Give two specific use cases where Unity is superior.

**Question 3** (Architecture): Explain why Unity Robotics Hub uses TCP instead of directly integrating with ROS 2 DDS. What are the trade-offs?

---

## Hands-On Exercises

### Exercise 1: Import and Visualize URDF

**Task**: Import the humanoid URDF from Chapter 4 into Unity and visualize joint states from ROS 2.

**Steps**:
1. Install Unity 2021.3 LTS + Unity Robotics Hub
2. Import `simple_humanoid.urdf` using URDF Importer
3. Launch ROS-TCP Endpoint: `ros2 run ros_tcp_endpoint default_server_endpoint`
4. Attach `RobotStateVisualizer.cs` script to robot
5. Publish joint states: `ros2 topic pub /joint_states ...`
6. Verify robot moves in Unity

**Expected Learning**: URDF import workflow, Unity-ROS 2 communication setup.

---

### Exercise 2: Create Photorealistic Environment

**Task**: Design a warehouse environment with realistic lighting and materials.

**Requirements**:
- Floor: 20×20m with concrete texture
- Walls: 3m high with painted surface
- Lighting: Skylights + warehouse lights (point lights)
- Objects: 5-10 boxes (random placement)
- Robot: Import humanoid, place at (0, 0, 0)

**Solution Guidance**:
- Use Unity primitives (Plane, Cube) or import models from Asset Store
- Apply PBR materials (metallic=0, smoothness=0.3 for concrete)
- Add HDRI skybox for ambient lighting
- Bake lightmaps for static shadows

---

### Exercise 3: Recorded Trajectory Playback

**Task**: Record a robot trajectory in Gazebo, replay it in Unity for visualization.

**Steps**:
1. Launch Gazebo with humanoid, run teleoperation (Chapter 5)
2. Record bag: `ros2 bag record /joint_states /tf`
3. Stop recording after 30 seconds of motion
4. Close Gazebo
5. Play bag: `ros2 bag play trajectory.db3 -l` (loop)
6. Launch Unity with robot visualization
7. Robot in Unity replays recorded motion

**Expected Learning**: ROS bag integration with Unity, offline visualization workflow.

---

## Key Takeaways

1. **Digital twin**: Virtual representation mirroring physical robot state in real-time via bidirectional data connection
2. **Unity for visualization**: Photorealistic rendering (HDRP, ray tracing), cross-platform (Windows/macOS/Linux/WebGL), AR/VR support
3. **Unity vs Gazebo**: Unity excels at rendering/demos, Gazebo excels at physics/testing → hybrid approach combines strengths
4. **Unity Robotics Hub**: ROS-TCP Connector (Unity C# plugin) + ROS-TCP Endpoint (Python node) bridge ROS 2 DDS ↔ Unity TCP
5. **URDF Importer**: Converts URDF to Unity GameObjects with ArticulationBody joints, handles Y-up vs Z-up axis conversion
6. **Photorealistic rendering**: HDRP with PBR materials, HDRI lighting, baked lightmaps for synthetic training data generation
7. **Synthetic data**: Unity Perception package generates labeled datasets (bounding boxes, segmentation, keypoints) with domain randomization
8. **Real-time vs playback**: Real-time for monitoring/teleoperation, playback (ROS bags) for debugging/analysis
9. **Hybrid simulation**: Gazebo physics + Unity visualization = accurate simulation + photorealistic presentation
10. **Cross-platform deployment**: Unity builds for desktop, WebGL (browser), mobile, AR/VR headsets

---

## References

(See Ch6 references file)

---

## Answer Key

**Answer 1**: **Three components**:
1. **Physical Asset**: Real robot hardware with sensors/actuators
2. **Virtual Model**: 3D simulation (Unity scene with URDF-imported robot, physics, rendering)
3. **Data Connection**: Bidirectional communication (ROS-TCP Connector + Endpoint)

**Data Flow**:
- Physical → Virtual: Sensor data (joint states, odometry, camera images) published to ROS topics → ROS-TCP Endpoint → TCP socket → Unity updates 3D model positions, renders scene
- Virtual → Physical: Commands (test trajectories, velocity commands) sent from Unity → ROS-TCP Endpoint → ROS topics → robot actuators execute

**Answer 2**: Choose **Unity over Gazebo** when:
1. **Photorealistic visualization for demos/marketing**: Unity's HDRP rendering with ray tracing, global illumination, PBR materials produces photorealistic output for stakeholder presentations, product demos, training videos (Gazebo has functional OGRE rendering unsuitable for marketing)
2. **AR/VR applications**: Unity has native AR (ARCore, ARKit, HoloLens) and VR (Quest, Vive) support for immersive teleoperation interfaces, spatial awareness for manipulation, human-robot collaboration visualization (Gazebo has no AR/VR support)

Bonus: Synthetic dataset generation with automatic labeling (Unity Perception package), WebGL deployment for browser-based monitoring

**Answer 3**: Unity uses **TCP instead of DDS** because:
1. **Cross-platform compatibility**: Unity runs on Windows/macOS where DDS multicast discovery often blocked by firewalls → TCP works through firewalls with simple port forwarding
2. **Simpler integration**: Unity is C# game engine, DDS has limited C# bindings → TCP socket simpler to implement
3. **Unity's threading model**: Unity is single-threaded for rendering; DDS callbacks from multiple threads would require complex synchronization

**Trade-offs**:
- **Latency**: TCP adds ~5-20ms latency vs. DDS shared memory (acceptable for visualization, problematic for real-time control)
- **Bandwidth**: TCP is point-to-point (one Unity instance per endpoint) vs. DDS multi-subscriber (many RViz instances can subscribe to same topic)
- **No discovery**: Must manually configure IP/port vs. DDS automatic discovery

---

**End of Chapter 6**

**Next Chapter Preview**: Chapter 7 will introduce NVIDIA Isaac Sim platform, covering Omniverse, GPU-accelerated physics, photorealistic sensors, and parallel environment training for reinforcement learning.

---

**Last Updated**: 2025-12-23
**Tested On**: Ubuntu 22.04 + Windows 11, Unity 2021.3 LTS, ROS 2 Humble
